{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch \n",
    "import gc \n",
    "import json \n",
    "from tokens import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:03<00:00,  1.24s/it]\n",
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "2024-11-07 17:54:01.205163: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-07 17:54:01.219567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-07 17:54:01.237155: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-07 17:54:01.242472: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-07 17:54:01.255171: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-07 17:54:03.195908: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer with the access token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=access_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=access_token\n",
    ")\n",
    "\n",
    "# Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeater(messages):\n",
    "\n",
    "    # Apply chat template to messages\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Define terminators\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "    ]\n",
    "\n",
    "    # Generate text\n",
    "    outputs = pipeline(\n",
    "        prompt,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.5,\n",
    "        top_p=0.9,\n",
    "        max_new_tokens=2024*2\n",
    "    )\n",
    "    return len(prompt), outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 are available.\n"
     ]
    }
   ],
   "source": [
    "with open('articles.json', 'r') as file: \n",
    "    articles = json.load(file)\n",
    "\n",
    "print(f'{len(articles[len(articles) - 1])} are available.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define messages\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an AI assistant tasked with producing NEUTRAL news article based on the provided article information.\"\n",
    "                                \"Try to filter the bias that maybe existent in the article\" \n",
    "                                \"Write a new article as a journalist, ensuring the content is unbiased, factual, and informative.\"},\n",
    "                                \n",
    "    {\"role\": \"user\", \"content\": articles[0][0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt_len, outputs = repeater(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: Starmer Updates Ministerial Gift and Hospitality Rules in Response to Public Scrutiny\n",
      "\n",
      "Keir Starmer, the Prime Minister, has tightened the rules regarding gifts and hospitality accepted by ministers, following a series of criticisms over the acceptance of freebies by senior Labour figures. The updated ministerial code now requires ministers to consider the \"need to maintain the public's confidence in the standards of propriety\" when deciding whether to accept gifts.\n",
      "\n",
      "The new regulations state that ministers should refrain from accepting gifts, hospitality, or services that may compromise their judgment or create an appearance of obligation. This principle extends to family members as well. However, the code acknowledges that ministers may need to attend events where hospitality is offered due to their governmental duties.\n",
      "\n",
      "The monthly register of gifts received by ministers, which will now be published, will include details and the value of gifts worth more than £140, as well as hospitality received and given by ministers in their ministerial capacity.\n",
      "\n",
      "This move comes after Sir Keir repaid over £6,000 worth of gifts and hospitality he received since becoming prime minister, including tickets to see Taylor Swift, following a public backlash. He has also faced scrutiny for receiving thousands of pounds of clothes from Labour peer and donor Lord Alli. In response, Downing Street has announced that Sir Keir, Deputy Prime Minister Angela Rayner, and Chancellor Rachel Reeves will no longer accept donations of clothes from Lord Alli.\n",
      "\n",
      "The updated ministerial code also includes the Prime Minister's decision to rent out his family home in north London after moving into Downing Street, a practice that has been followed by previous prime ministers, such as David Cameron and Theresa May. The list of ministers' interests, which details any relevant private interests that could potentially conflict with a minister's public duties, will now be published quarterly rather than twice a year.."
     ]
    }
   ],
   "source": [
    "print(outputs[0][\"generated_text\"][prompt_len:], end='.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "del outputs\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
