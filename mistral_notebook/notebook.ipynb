{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch \n",
    "import gc \n",
    "from tokens import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [03:33<00:00, 71.06s/it]\n",
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:796: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "2024-10-24 16:14:23.811985: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-24 16:14:23.825894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-24 16:14:23.842696: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-24 16:14:23.847792: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-24 16:14:23.860871: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-24 16:14:26.794253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Load the model and tokenizer with the access token\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    mistral_model_id,\n",
    "    use_auth_token=mistral_access_token,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_4bit=True\n",
    ")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    use_auth_token=access_token\n",
    ")\n",
    "\n",
    "# Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeater(question):\n",
    "  # Define messages\n",
    "#   messages = [\n",
    "#       {\"role\": \"system\", \"content\": \"You are an AI assistant tasked with producing neutral news articles based on the provided topics. Ensure the content is unbiased, factual, and informative, and make sure to conclude the article logically.\"},\n",
    "#       {\"role\": \"user\", \"content\": question}\n",
    "#   ]\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": \"You are an AI assistant tasked with producing NEUTRAL news article based on the provided article information.\"\n",
    "                                    \"- Try to filter the political bias that maybe existent in the article\"\n",
    "                                    \"- Don't create new information that is not existent in the article\" \n",
    "                                    \"- Write a new article as a journalist, ensuring the content is unbiased, factual, and informative.\"},\n",
    "                                    \n",
    "      {\"role\": \"user\", \"content\": question}\n",
    "  ]\n",
    "  # Apply chat template to messages\n",
    "  prompt = pipeline.tokenizer.apply_chat_template(\n",
    "      messages,\n",
    "      tokenize=False,\n",
    "      add_generation_prompt=True\n",
    "  )\n",
    "\n",
    "  # Define terminators\n",
    "  terminators = [\n",
    "      pipeline.tokenizer.eos_token_id,\n",
    "  ]\n",
    "\n",
    "  # Generate text\n",
    "  outputs = pipeline(\n",
    "      prompt,\n",
    "      eos_token_id=terminators,\n",
    "      do_sample=True,\n",
    "      temperature=0.5,\n",
    "      top_p=0.9,\n",
    "      max_new_tokens=2024*2\n",
    "  )\n",
    "\n",
    "  with open('mistral_output.txt', 'w') as file: \n",
    "    file.write(outputs[0][\"generated_text\"][len(prompt):])\n",
    "\n",
    "  del outputs\n",
    "  gc.collect()\n",
    "  torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt','r') as f:\n",
    "  content = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "repeater(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
