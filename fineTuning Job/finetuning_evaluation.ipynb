{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "from functions import *\n",
    "from tokens import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset: 24034 examples\n",
      "Eval Dataset: 6009 examples\n",
      "Test Dataset: 7511 examples\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"LOAD DATASET\"\"\"\n",
    "dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset')\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "eval_dataset = dataset[\"eval\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "print(f\"Train Dataset: {len(train_dataset)} examples\")\n",
    "print(f\"Eval Dataset: {len(eval_dataset)} examples\")\n",
    "print(f\"Test Dataset: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/t/tzelilai/Desktop/Thesis/results/checkpoint-4000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-1B\"  # same as in your adapter_config.json\n",
    "adapter_path = \"/home/t/tzelilai/Desktop/Thesis/results/checkpoint-4000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the *base* LLaMA model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        use_auth_token=access_token,\n",
    "        quantization_config = bnb_config,\n",
    "        num_labels=3,\n",
    "        device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# 2. Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'Monday brought the worst coronavirus crash so far, sending stock markets plunging in a freefall that led to the biggest drop since 2008. Now President Trump is proposing a step to stop the slide even as a bit of good news appears to also be coming out of China.\\n\\nThe White House is set to ask Congress for a payroll tax cut and economic assistance to hourly workers to help those hurt financially by the coronavirus.\\n\\n\"We are to be meeting with House Republicans, Mitch McConnell, and discussing a possible payroll tax cut or relief, substantial relief,\" President Trump said Monday during a news conference at the White House.\\n\\nFinancial experts say the relief could be a boost to public confidence.\\n\\n\"Yes, it would make a difference,\" Dan Celia, president and CEO of Financial Issues Stewardship Ministries, told CBN News, warning that the coronavirus fears are actually undermining economic strength. \"Now it\\'s beginning to hurt the economy in a bigger way – that will make a huge difference.\"\\n\\nDid Markets Bottom Out on Monday?\\n\\nStock markets in Asia recovered some of their losses Tuesday following their worst day since the 2008 financial crisis. Oil prices also bounced back from a record-setting fall.\\n\\nThe Dow fell more than 2,000 points Monday, triggering a halt on trading for the first time in two decades after the market plunged seven percent in just a few minutes.\\n\\nREAD Why Trading Curbs Kicked in Monday as Stocks Plunged and What You Should Do About It\\n\\n\"The markets over time have up until now always recovered, it might not be next month or next quarter, but we need to keep, if it is for retirement purposes, we need to keep that long term picture in mind,\" said Joel Griffith with the Heritage Foundation.\\n\\nStill, analysts expect markets around the world to continue bouncing in and out of negative territory for some time.\\n\\n\"So don\\'t go into the market and go bottom fishing, because the bottom is still yet to come,\" said Francis Lun with Hong Kong-based Geo Securities Limited.\\n\\nQuarantine Measures from the White House to Italy\\n\\nIn Washington, the president\\'s new chief of staff Mark Meadows has self-quarantined after possibly coming into contact with a person infected with coronavirus.\\n\\nSeveral other Republican lawmakers, who met with the president, also quarantining themselves out of an abundance of caution.\\n\\nGET YOUR FREE FACTSHEET:\\n\\nCoronavirus: What You Need to Know\\n\\nMeanwhile, the streets of Florence, Italy were practically empty this morning after the government there placed the entire country of some 60 million people under quarantine.\\n\\n\\n\\nOvernight, residents of the Italian capital stood in long lines stockpiling food as schools, universities, restaurants and all social gatherings have been banned until April 3rd.\\n\\n\"It is right at this time to stay at home,\" said Italy\\'s prime minister Giuseppe Conte. \"Our future, the future of Italy is in our hands and our hands have to be responsible hands, today more than ever. Everyone has to do their part.\"\\n\\nNine thousand cases of the coronavirus have been confirmed in Italy, and 463 people there have died so far – the largest numbers outside of China.\\n\\nGood Signs Emerge from China\\n\\nIn Wuhan, China, the epicenter of the virus outbreak, some patients who had the most severe cases, are now recovering.\\n\\nChinese state TV showed a 56-year-old man being transferred from the ICU to the ordinary ward.\\n\\n\"I\\'ve passed the severe period,\" said Mr. Zhu, a COVID-19 patient. \"It means I\\'ve defeated the disease.\"\\n\\nBeijing saying its efforts to control the virus are working as government officials reported the lowest number of new cases since January.\\n\\nChinese President Xi Jinping visited medical workers and patients in Wuhan for the first time today in a signal that perhaps the country has finally brought the outbreak under control.\\n\\n\"If we can see a country have more than 80,000 cases now start to see a decline, that is more than hope,\" said the World Health Organization\\'s Maria van Kerkhove during a press briefing. \"That is evidence showing that this can be done.\"\\n\\nGET YOUR FREE FACTSHEET:\\n\\nCoronavirus: What You Need to Know', 'labels': 2}\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6009\n"
     ]
    }
   ],
   "source": [
    "print(len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_2', 'score': 0.9999997615814209}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline(eval_dataset[3]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LLM is:  0.9526028491545733\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "\n",
    "for article in test_dataset: \n",
    "    llm_predict = pipeline(article['content'])\n",
    "    llm_label = llm_predict[0]['label']\n",
    "\n",
    "    if article['labels'] == 0:\n",
    "        article_label = 'LABEL_0'\n",
    "    elif article['labels'] == 1: \n",
    "        article_label = 'LABEL_1'\n",
    "    else:\n",
    "        article_label = 'LABEL_2'\n",
    "\n",
    "    if article_label == llm_label: \n",
    "        correct_predictions += 1 \n",
    "\n",
    "accuracy = correct_predictions / len(test_dataset)\n",
    "print(\"Accuracy of LLM is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of LLM is:  0.9582293226826427\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_predictions / len(eval_dataset)\n",
    "print(\"Accuracy of LLM is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct_predictions / len(eval_dataset)\n",
    "print(\"Accuracy of LLM is: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = \"\"\"Title: Trump Taps Elon Musk to Lead Department of Government Efficiency, Amid Concerns Over Conflicts of Interest\n",
    "\n",
    "President-elect Donald Trump has appointed Elon Musk, the CEO of Tesla and SpaceX, to lead a newly created Department of Government Efficiency (DOGE), a non-governmental body tasked with slashing government spending and streamlining federal agencies. While Cathie Wood, CEO of ARK Invest, backs Musk's appointment, citing his ability to bring proprietary data to the table, others are raising concerns over potential conflicts of interest.\n",
    "\n",
    "Musk's companies, which include SpaceX, Tesla, and Neuralink, have received billions of dollars in federal contracts over the past decade, and he has been vocal about his desire to reduce regulations and cut government spending. However, critics argue that his appointment could create conflicts of interest, as he may be in a position to benefit from cost-cutting measures that affect his own companies.\n",
    "\n",
    "The concerns are not new, as Trump previously tapped billionaire Carl Icahn to streamline government regulation in 2017. Icahn resigned after seven months amid ethics questions over changes to an energy policy that could have benefited his own company. Musk's appointment has raised similar concerns, with experts citing his vast portfolio across multiple sectors and his potential to benefit from cost-cutting measures.\n",
    "\n",
    "Wood, however, believes Musk's understanding of the current technological landscape gives him a unique perspective on the government's role in the private sector. \"The way we're looking at this is, Elon understands we are at the threshold of a convergence among many technologies,\" she said. \"AI being at the center of it, and proprietary data is winning. So he has more proprietary data from all of these companies than I think any other CEO.\"\n",
    "\n",
    "The specifics of how the DOGE will operate are still unclear, but Trump has stated that it will \"slash excess regulations, cut wasteful expenditures, and restructure federal agencies\" and \"provide advice and guidance from outside of government.\" Musk has said he aims to cut $2 trillion from the federal budget, and will work with the White House and Office of Management and Budget to achieve this goal.\n",
    "\n",
    "While the appointment has raised concerns, Musk's supporters see him as a visionary leader who can bring about positive change. \"We've faced this question about Elon for many years, as he started one company after another,\" Wood said. \"We've seen him overcome incredible odds in his business career.\"\n",
    "\n",
    "As the DOGE gets underway, concerns over conflicts of interest will continue to be a topic of discussion. However, for now, Musk and Ramaswamy are eager to get to work, seeking \"super high-IQ small-government revolutionaries\" to join their team and help drive large-scale structural reform..\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_article = pipeline(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.8408420085906982}]\n"
     ]
    }
   ],
   "source": [
    "print(llm_article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/llama3.1_notebook/example_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You need to specify either `text` or `text_target`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLabel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mllm_dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:159\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 159\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/base.py:1302\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1296\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m     )\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/base.py:1308\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[0;32m-> 1308\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1309\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1310\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n",
      "File \u001b[0;32m~/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:171\u001b[0m, in \u001b[0;36mTextClassificationPipeline.preprocess\u001b[0;34m(self, inputs, **tokenizer_kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m return_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# It used to be valid to use a list of list of list for text pairs, keeping this path for BC\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(\n\u001b[1;32m    175\u001b[0m         text\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m], text_pair\u001b[38;5;241m=\u001b[39minputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs\n\u001b[1;32m    176\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/tokenization_utils_base.py:3015\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3013\u001b[0m all_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   3014\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 3015\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to specify either `text` or `text_target`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3017\u001b[0m     \u001b[38;5;66;03m# The context manager will send the inputs as normal texts and not text_target, but we shouldn't change the\u001b[39;00m\n\u001b[1;32m   3018\u001b[0m     \u001b[38;5;66;03m# input mode in this case.\u001b[39;00m\n\u001b[1;32m   3019\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n",
      "\u001b[0;31mValueError\u001b[0m: You need to specify either `text` or `text_target`."
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    prediction = pipeline(llm_dataset[i]['llm_article'])\n",
    "    print(f\"Label: {llm_dataset[i]['labels']} | Prediction: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
