{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-13 11:27:56.594270: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-13 11:27:56.610317: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736760476.629646 1062575 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736760476.635623 1062575 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-13 11:27:56.655718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "from functions import *\n",
    "from tokens import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOAD DATASET\"\"\"\n",
    "dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-1B\"  # same as in your adapter_config.json\n",
    "adapter_path = \"/home/t/tzelilai/Desktop/Thesis/results-modified_articles/checkpoint-4506\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the *base* LLaMA model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        use_auth_token=access_token,\n",
    "        quantization_config = bnb_config,\n",
    "        num_labels=3,\n",
    "        device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# 2. Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Simple Prompt Outputs\n",
    "import json \n",
    "outputs_path = [None for _ in range(6)]\n",
    "outputs = [None for _ in range(6)]\n",
    "for i in range(6):\n",
    "    outputs_path[i] = \"/home/t/tzelilai/Desktop/Thesis/llama3.1_notebook/test_outputs_\" +str(i)+\".json\"\n",
    "    with open(outputs_path[i], 'r') as file: \n",
    "        outputs[i] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    }
   ],
   "source": [
    "left_counter = 0\n",
    "center_counter = 0\n",
    "right_counter = 0 \n",
    "llama_articles = [[] for i in range(6)]\n",
    "\n",
    "for i,batch in enumerate(outputs):\n",
    "    for article in batch: \n",
    "        llm_predict = pipeline(article, return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        llama_articles[i].append(llm_predict[0])\n",
    "        # if llm_label == \"LABEL_0\":\n",
    "        #     left_counter += 1 \n",
    "        # elif llm_label == \"LABEL_1\":\n",
    "        #     center_counter += 1 \n",
    "        # else: \n",
    "        #     right_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"llama3.1_outputs.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(llama_articles, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "label_counts = [None for _ in range(6)]\n",
    "total_label_count = Counter()\n",
    "for i in range(6):\n",
    "    label_counts[i] = Counter(dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1620\n",
      "Total Center Content: 1746\n",
      "Total Right Counter: 2250\n",
      "----------------------------\n",
      "Left Classified: 1349\n",
      "Center Classified  3455\n",
      "Right Classified 816\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1620\n",
      "Total Center Content: 1746\n",
      "Total Right Counter: 2250\n",
      "----------------------------\n",
      "Left Classified: 1169\n",
      "Center Classified  2380\n",
      "Right Classified 1140\n"
     ]
    }
   ],
   "source": [
    "# Prompt with more instructions\n",
    "from collections import Counter\n",
    "label_counts = [None for _ in range(6)]\n",
    "total_label_count = Counter()\n",
    "for i in range(6):\n",
    "    label_counts[i] = Counter(dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]\n",
    "\n",
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral Outputs \n",
    "import json \n",
    "mistral_outputs_path = [None for _ in range(8)]\n",
    "mistral_outputs = [None for _ in range(8)]\n",
    "for i in range(0,8,1):\n",
    "    mistral_outputs_path[i] = \"/home/t/tzelilai/Desktop/Thesis/mistral_notebook/test_outputs_\" +str(i)+\"_new_prompt\" +\".json\"\n",
    "    with open(mistral_outputs_path[i], 'r') as file: \n",
    "        mistral_outputs[i] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "left_counter = 0\n",
    "center_counter = 0\n",
    "right_counter = 0 \n",
    "mistral_articles = [[] for i in range(8)]\n",
    "for i,batch in enumerate(mistral_outputs):\n",
    "    for article in batch: \n",
    "        llm_predict = pipeline(article, return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        mistral_articles[i].append(llm_predict[0])\n",
    "\n",
    "        # if llm_label == \"LABEL_0\":\n",
    "        #     left_counter += 1 \n",
    "        # elif llm_label == \"LABEL_1\":\n",
    "        #     center_counter += 1 \n",
    "        # else: \n",
    "        #     right_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"mistral_outputs.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(mistral_articles, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted-mistral/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_articles = [int(label.split('_')[-1]) for label in mistral_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mistral_dataset['0']['labels'][232])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.0022257075179368258}, {'label': 'LABEL_1', 'score': 0.4595065116882324}, {'label': 'LABEL_2', 'score': 0.5382677316665649}]]\n",
      "[{'label': 'LABEL_2', 'score': 0.5382677316665649}]\n",
      "[{'label': 'LABEL_2', 'score': 0.5382677316665649}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline(mistral_outputs[0][0], return_all_scores=True))\n",
    "print(pipeline(mistral_outputs[0][0]))\n",
    "print(pipeline(mistral_outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "true_negative = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "false_positive = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "false_negative = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "\n",
    "i = 0\n",
    "j = 0 \n",
    "k = 0 \n",
    "while i < len(mistral_articles) and j < len(mistral_dataset): \n",
    "\n",
    "    if mistral_articles[i] == mistral_dataset[str(j)]['labels'][k]:\n",
    "        if mistral_articles[i] == 0: \n",
    "            true_positive['label_0'] += 1 \n",
    "        elif mistral_articles[i] == 1: \n",
    "            true_positive['label_1'] += 1 \n",
    "        else: \n",
    "            true_positive['label_2'] += 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1640\n",
      "Total Center Content: 1808\n",
      "Total Right Counter: 2168\n",
      "----------------------------\n",
      "Left Classified: 1035\n",
      "Center Classified  3254\n",
      "Right Classified 1331\n"
     ]
    }
   ],
   "source": [
    "# Prompt with more instructions\n",
    "from collections import Counter\n",
    "label_counts = [None for _ in range(8)]\n",
    "total_label_count = Counter()\n",
    "for i in range(8):\n",
    "    label_counts[i] = Counter(mistral_dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]\n",
    "\n",
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ORIGINAL DATASET\"\"\"\n",
    "dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataset):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m article \u001b[38;5;129;01min\u001b[39;00m batch: \n\u001b[0;32m----> 6\u001b[0m         llm_predict \u001b[38;5;241m=\u001b[39m pipeline(\u001b[43marticle\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, return_all_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# llm_label = llm_predict[0]['label']\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         original_articles[i]\u001b[38;5;241m.\u001b[39mappend(llm_predict[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "original_articles = [[] for i in range(6)]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    for article in dataset[str(i)]: \n",
    "        llm_predict = pipeline(article['content'], return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        original_articles[i].append(llm_predict[0])\n",
    "    #     if article['labels'] == 0:\n",
    "    #         article_label = 'LABEL_0'\n",
    "    #     elif article['labels'] == 1: \n",
    "    #         article_label = 'LABEL_1'\n",
    "    #     else:\n",
    "    #         article_label = 'LABEL_2'\n",
    "\n",
    "    #     if article_label == llm_label: \n",
    "    #         correct_predictions += 1 \n",
    "\n",
    "    # accuracy = correct_predictions / len(dataset)\n",
    "    # print(\"Accuracy of LLM is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"original_article_outputs.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(original_articles, file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
