{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig\n",
    "from peft import PeftModel\n",
    "from functions import *\n",
    "from tokens import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "load_in_4bit = True\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "bnb_4bit_use_double_quant = True\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Compute data type for 4-bit base models\n",
    "bnb_4bit_compute_dtype = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = create_bnb_config(load_in_4bit, bnb_4bit_use_double_quant, bnb_4bit_quant_type, bnb_4bit_compute_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"LOAD DATASET\"\"\"\n",
    "dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = \"meta-llama/Llama-3.2-1B\"  # same as in your adapter_config.json\n",
    "adapter_path = \"/home/t/tzelilai/Desktop/Thesis/results-modified_articles/checkpoint-4506\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the *base* LLaMA model for sequence classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        base_model_name,\n",
    "        use_auth_token=access_token,\n",
    "        quantization_config = bnb_config,\n",
    "        num_labels=3,\n",
    "        device_map = \"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# 2. Load the LoRA adapter on top of the base model\n",
    "model = PeftModel.from_pretrained(model, adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GlmForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PhimoeForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification', 'ZambaForSequenceClassification'].\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline with the specified model and tokenizer\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-classification\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama Simple Prompt Outputs\n",
    "import json \n",
    "outputs_path = [None for _ in range(6)]\n",
    "outputs = [None for _ in range(6)]\n",
    "for i in range(6):\n",
    "    outputs_path[i] = \"/home/t/tzelilai/Desktop/Thesis/llama3.1_notebook/test_outputs_\" +str(i)+\"_new_prompt\"+\".json\"\n",
    "    with open(outputs_path[i], 'r') as file: \n",
    "        outputs[i] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "left_counter = 0\n",
    "center_counter = 0\n",
    "right_counter = 0 \n",
    "llama_articles = [[] for i in range(6)]\n",
    "\n",
    "for i,batch in enumerate(outputs):\n",
    "    for article in batch: \n",
    "        llm_predict = pipeline(article, return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        llama_articles[i].append(llm_predict[0])\n",
    "        # if llm_label == \"LABEL_0\":\n",
    "        #     left_counter += 1 \n",
    "        # elif llm_label == \"LABEL_1\":\n",
    "        #     center_counter += 1 \n",
    "        # else: \n",
    "        #     right_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"llama3.1_outputs_2.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(llama_articles, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "label_counts = [None for _ in range(6)]\n",
    "total_label_count = Counter()\n",
    "for i in range(6):\n",
    "    label_counts[i] = Counter(dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1620\n",
      "Total Center Content: 1746\n",
      "Total Right Counter: 2250\n",
      "----------------------------\n",
      "Left Classified: 1349\n",
      "Center Classified  3455\n",
      "Right Classified 816\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1620\n",
      "Total Center Content: 1746\n",
      "Total Right Counter: 2250\n",
      "----------------------------\n",
      "Left Classified: 1169\n",
      "Center Classified  2380\n",
      "Right Classified 1140\n"
     ]
    }
   ],
   "source": [
    "# Prompt with more instructions\n",
    "from collections import Counter\n",
    "label_counts = [None for _ in range(6)]\n",
    "total_label_count = Counter()\n",
    "for i in range(6):\n",
    "    label_counts[i] = Counter(dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]\n",
    "\n",
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mistral Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mistral Outputs \n",
    "import json \n",
    "mistral_outputs_path = [None for _ in range(8)]\n",
    "mistral_outputs = [None for _ in range(8)]\n",
    "for i in range(0,8,1):\n",
    "    mistral_outputs_path[i] = \"/home/t/tzelilai/Desktop/Thesis/mistral_notebook/test_outputs_\" +str(i) +\".json\"\n",
    "    with open(mistral_outputs_path[i], 'r') as file: \n",
    "        mistral_outputs[i] = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "left_counter = 0\n",
    "center_counter = 0\n",
    "right_counter = 0 \n",
    "mistral_articles = [[] for i in range(8)]\n",
    "for i,batch in enumerate(mistral_outputs):\n",
    "    for article in batch: \n",
    "        llm_predict = pipeline(article, return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        mistral_articles[i].append(llm_predict[0])\n",
    "\n",
    "        # if llm_label == \"LABEL_0\":\n",
    "        #     left_counter += 1 \n",
    "        # elif llm_label == \"LABEL_1\":\n",
    "        #     center_counter += 1 \n",
    "        # else: \n",
    "        #     right_counter += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"mistral_outputs.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(mistral_articles, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted-mistral/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistral_articles = [int(label.split('_')[-1]) for label in mistral_articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mistral_dataset['0']['labels'][232])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'label': 'LABEL_0', 'score': 0.0022257075179368258}, {'label': 'LABEL_1', 'score': 0.4595065116882324}, {'label': 'LABEL_2', 'score': 0.5382677316665649}]]\n",
      "[{'label': 'LABEL_2', 'score': 0.5382677316665649}]\n",
      "[{'label': 'LABEL_2', 'score': 0.5382677316665649}]\n"
     ]
    }
   ],
   "source": [
    "print(pipeline(mistral_outputs[0][0], return_all_scores=True))\n",
    "print(pipeline(mistral_outputs[0][0]))\n",
    "print(pipeline(mistral_outputs[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_positive = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "true_negative = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "false_positive = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "false_negative = {'label_0':0, 'label_1':0, 'label_2':0}\n",
    "\n",
    "i = 0\n",
    "j = 0 \n",
    "k = 0 \n",
    "while i < len(mistral_articles) and j < len(mistral_dataset): \n",
    "\n",
    "    if mistral_articles[i] == mistral_dataset[str(j)]['labels'][k]:\n",
    "        if mistral_articles[i] == 0: \n",
    "            true_positive['label_0'] += 1 \n",
    "        elif mistral_articles[i] == 1: \n",
    "            true_positive['label_1'] += 1 \n",
    "        else: \n",
    "            true_positive['label_2'] += 1 \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Left Content: 1640\n",
      "Total Center Content: 1808\n",
      "Total Right Counter: 2168\n",
      "----------------------------\n",
      "Left Classified: 1035\n",
      "Center Classified  3254\n",
      "Right Classified 1331\n"
     ]
    }
   ],
   "source": [
    "# Prompt with more instructions\n",
    "from collections import Counter\n",
    "label_counts = [None for _ in range(8)]\n",
    "total_label_count = Counter()\n",
    "for i in range(8):\n",
    "    label_counts[i] = Counter(mistral_dataset['0']['labels'])\n",
    "    total_label_count += label_counts[i]\n",
    "\n",
    "print(\"Total Left Content:\",total_label_count[0])\n",
    "print(\"Total Center Content:\",total_label_count[1])\n",
    "print(\"Total Right Counter:\",total_label_count[2])\n",
    "print(\"----------------------------\")\n",
    "print(\"Left Classified:\",left_counter)\n",
    "print(\"Center Classified \",center_counter)\n",
    "print(\"Right Classified\", right_counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"ORIGINAL DATASET\"\"\"\n",
    "dataset = load_from_disk('/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "936\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset['0']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = 0\n",
    "original_articles = [[] for i in range(6)]\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    for article in dataset[str(i)]: \n",
    "        llm_predict = pipeline(article['content'], return_all_scores=True)\n",
    "        # llm_label = llm_predict[0]['label']\n",
    "        original_articles[i].append(llm_predict[0])\n",
    "    #     if article['labels'] == 0:\n",
    "    #         article_label = 'LABEL_0'\n",
    "    #     elif article['labels'] == 1: \n",
    "    #         article_label = 'LABEL_1'\n",
    "    #     else:\n",
    "    #         article_label = 'LABEL_2'\n",
    "\n",
    "    #     if article_label == llm_label: \n",
    "    #         correct_predictions += 1 \n",
    "\n",
    "    # accuracy = correct_predictions / len(dataset)\n",
    "    # print(\"Accuracy of LLM is: \", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open(\"original_article_outputs.json\", \"w\", encoding=\"utf-8\") as file: \n",
    "    json.dump(original_articles, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5620\n"
     ]
    }
   ],
   "source": [
    "with open(\"mistral_outputs.json\", \"r\") as file: \n",
    "    data = json.load(file)\n",
    "\n",
    "total_articles = 0\n",
    "for batch in data: \n",
    "    total_articles += len(batch)\n",
    "print(total_articles)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
