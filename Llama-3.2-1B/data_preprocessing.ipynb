{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/t/tzelilai/Desktop/Thesis/venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os, json \n",
    "from datasets import DatasetDict, ClassLabel, Dataset, load_from_disk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path of articles in json format \n",
    "articles_path = '/home/t/tzelilai/Desktop/Thesis/Article-Bias-Prediction-main/data/jsons'\n",
    "topics = set()\n",
    "\"\"\"(json_data[\"topic\"] == \"politics\" or json_data[\"topic\"] == \"elections\" or json_data[\"topic\"] == \"abortions\")\"\"\"\n",
    "accept_topics = {\"politics\"}\n",
    "# List to store data \n",
    "data = []\n",
    "i = 0 \n",
    "j = 0 \n",
    "# Read each JSON file and append its content\n",
    "for file_name in os.listdir(articles_path):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(articles_path, file_name), \"r\") as f:\n",
    "            i += 1\n",
    "            json_data = json.load(f)   \n",
    "                 # 4500     #and json_data[\"topic\"] in accept_topics\n",
    "            word_count = len(json_data[\"content\"].split())\n",
    "            if len(json_data[\"content\"]) <= 12000 :\n",
    "                topics.add(json_data[\"topic\"])\n",
    "                j += 1\n",
    "                format_data = {\"content\":json_data['content'], \"labels\":json_data[\"bias_text\"]}\n",
    "                data.append(format_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of articles checked: 37554\n",
      "number of articles accepted: 35504\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of articles checked: {i}\")\n",
    "print(f\"number of articles accepted: {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12726 10731 13658\n"
     ]
    }
   ],
   "source": [
    "left = 0\n",
    "center = 0\n",
    "right = 0 \n",
    "\n",
    "for article in data: \n",
    "    if article[\"labels\"] == \"left\":\n",
    "        left+=1\n",
    "    elif article[\"labels\"] == \"center\":\n",
    "        center+=1\n",
    "    elif article[\"labels\"] == \"right\":\n",
    "        right+=1\n",
    "\n",
    "print(left, center, right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New balanced dataset size: 12840\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Suppose `data` is a list of articles, each article being a dictionary or similar structure.\n",
    "# We'll split them by label into three separate lists:\n",
    "left_articles = []\n",
    "center_articles = []\n",
    "right_articles = []\n",
    "\n",
    "for article in data:\n",
    "    if article[\"labels\"] == \"left\":\n",
    "        left_articles.append(article)\n",
    "    elif article[\"labels\"] == \"center\":\n",
    "        center_articles.append(article)\n",
    "    elif article[\"labels\"] == \"right\":\n",
    "        right_articles.append(article)\n",
    "\n",
    "# Shuffle each list to ensure randomness\n",
    "random.shuffle(left_articles)\n",
    "random.shuffle(center_articles)\n",
    "random.shuffle(right_articles)\n",
    "\n",
    "# Find the smallest class size\n",
    "min_size = min(len(left_articles), len(center_articles), len(right_articles))\n",
    "\n",
    "# Now sample each list to the min_size\n",
    "left_balanced = left_articles[:min_size]\n",
    "center_balanced = center_articles[:min_size]\n",
    "right_balanced = right_articles[:min_size]\n",
    "\n",
    "# Combine them back into a single list\n",
    "balanced_data_undersampled = left_balanced + center_balanced + right_balanced\n",
    "\n",
    "random.shuffle(balanced_data_undersampled)\n",
    "\n",
    "print(f\"New balanced dataset size: {len(balanced_data_undersampled)}\")\n",
    "\n",
    "# If you want `data` itself to hold the balanced articles:\n",
    "data = balanced_data_undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left: 4280\n",
      "Center: 4280\n",
      "Right: 4280\n"
     ]
    }
   ],
   "source": [
    "print(\"Left:\", len(left_balanced))\n",
    "print(\"Center:\", len(center_balanced))\n",
    "print(\"Right:\", len(right_balanced))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extract the labels\n",
    "labels = [item[\"labels\"] for item in data]\n",
    "\n",
    "# Split the data list into training and testing\n",
    "train_data, test_data = train_test_split(data, test_size=0.15, random_state=42,stratify=labels)\n",
    "\n",
    "# 1) Extract the labels\n",
    "train_labels = [item[\"labels\"] for item in train_data]\n",
    "train_data, eval_data = train_test_split(train_data, test_size=0.15, random_state=42, stratify=train_labels)\n",
    "\n",
    "# Create Dataset objects for train and test splits\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "eval_dataset = Dataset.from_list(eval_data)\n",
    "test_dataset = Dataset.from_list(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 9276\n",
      "Eval:  1638\n",
      "Test:  1926\n"
     ]
    }
   ],
   "source": [
    "print(\"Training:\", len(train_dataset))\n",
    "print(\"Eval: \", len(eval_dataset))\n",
    "print(\"Test: \", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9276/9276 [00:01<00:00, 8158.83 examples/s] \n",
      "Map: 100%|██████████| 1638/1638 [00:00<00:00, 8643.83 examples/s] \n",
      "Map: 100%|██████████| 1926/1926 [00:00<00:00, 12084.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Define the ClassLabel\n",
    "class_label = ClassLabel(num_classes=3, names=[\"left\", \"center\", \"right\"])\n",
    "\n",
    "# Map the labels to ClassLabel integers for each dataset\n",
    "def encode_labels(example):\n",
    "    example[\"labels\"] = class_label.str2int(example[\"labels\"])\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(encode_labels)\n",
    "eval_dataset = eval_dataset.map(encode_labels)\n",
    "test_dataset = test_dataset.map(encode_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train and test datasets into a DatasetDict\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset, \"eval\":eval_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9276/9276 [00:00<00:00, 108977.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1926/1926 [00:00<00:00, 65492.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1638/1638 [00:00<00:00, 66457.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/home/t/tzelilai/Desktop/Thesis/Datasets/4500_words_evenly_splitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk \n",
    "\n",
    "dataset = load_from_disk(\"/home/t/tzelilai/Desktop/Thesis/Datasets/4500_words_evenly_splitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(test_dataset)\n",
    "size = 6\n",
    "part_size = n // size\n",
    "splits = []\n",
    "\n",
    "for i in range(size):\n",
    "    start = i * part_size\n",
    "    end = (i + 1) * part_size\n",
    "    # Handle any remainder in the last split if n isn't perfectly divisible by 8:\n",
    "    if i == size - 1: \n",
    "        end = n \n",
    "    split_i = test_dataset.select(range(start, end))\n",
    "    splits.append(split_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "}), Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "}), Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "}), Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "}), Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "}), Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 321\n",
      "})]\n"
     ]
    }
   ],
   "source": [
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the train and test datasets into a DatasetDict\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"test\": DatasetDict({str(i): test_split for i,test_split in enumerate(splits)}), \"eval\":eval_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/9276 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 9276/9276 [00:00<00:00, 109253.17 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 20969.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 20045.43 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 23974.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 21752.51 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 20267.83 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 321/321 [00:00<00:00, 23601.92 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1638/1638 [00:00<00:00, 70506.25 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"4500_words_evenly_splitted_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from disk\n",
    "test_sp_dataset = load_from_disk(\"/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['content', 'labels'],\n",
      "    num_rows: 936\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_sp_dataset['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = load_from_disk(\"/home/t/tzelilai/Desktop/Thesis/Llama-3.2-1B/articles_dataset_les-than-7000-tokens-splitted/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': \"Washington ( CNN ) Americans are increasingly unhappy with President Barack Obama 's handling of ISIS , and a growing share of the nation believes that fight is going badly , according to a new CNN/ORC survey released Monday .\\nFifty-seven percent disapprove of his handling of foreign affairs more broadly , and 54 % disapprove of how the President is handling terrorism . Another 60 % rate Obama negatively on his handling of electronic national security .\\nThe declining approval ratings for Obama on national security come as a weekend of international turmoil further underscores the growing threats abroad .\\nAnd Egypt launched a second round of airstrikes against Islamic State strongholds in Libya on Monday , in retaliation for a video released Sunday that appeared to show ISIS militants beheading a group of 21 Egyptian Christians .\\nObama issued a statement condemning the killing of the Christians on Sunday night , though Obama 's Republican opponents have consistently made the case that the growing Islamic State threat is exacerbated by what they see as his weak leadership .\\nIn the poll , Americans increasingly believe the U.S. military action against ISIS is going badly , with 58 % saying so in the latest survey , up from 49 % who said the fight was n't going well in October .\\nEven among Democrats , nearly half — 46 % — say things are n't going well in the battle against ISIS .\\nAnd about half of respondents , 51 % , say they trust the President as Commander-in-Chief of the military .\\nBut with ISIS affiliates continuing to commit brutal , gruesome murders and multiple terrorist attacks abroad grabbing international headlines over the past few months , support for sending ground troops to Iraq and Syria to confront the threat appears to be growing .\\nThe survey suggests Americans are warming up to the idea of sending ground troops to combat the terrorist organization .\\nIn November , just 43 % supported deploying ground troops , while 55 % of Americans opposed it ; now the number in support has ticked up to 47 % , the highest level of support yet measured , with just half of Americans opposed .\\nStill , the parties have become more polarized on the prospect since November , with 61 % of Democrats opposed and a similar majority of Republicans supportive of the prospect , an eight-point increase . Independents , meanwhile , are split , with 48 % in favor and 50 % opposed .\\nThe prospect of sending in ground troops remains a sticking point for both congressional Democrats and Republicans in the debate over Obama 's Authorization for the Use of Military Force , which would give him legal authority to combat ISIS .\\nBut the AUMF , and Obama 's decision to go to Congress for the official authority to continue battling ISIS , is widely popular , according to the new poll .\\nSeventy-eight percent of Americans say Congress should give Obama the authority to fight ISIS , a slight decline from 82 % who supported it in December . A similarly large majority say Obama was right to ask Congress for the authority , rather than proceeding with the battle unilaterally .\\nThe survey was conducted among 1,027 adult Americans from Feb. 12-15 , and has a margin of sampling error of 3 % .\", 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "print(my_dataset['0'][935])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
